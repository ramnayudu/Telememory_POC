{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Telememory_POC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFo30hUv0hx1bnPW4azHeP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnayudu/Telememory_POC/blob/main/Telememory_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Z_qwOvb1MR"
      },
      "source": [
        "# Telememory POC \n",
        "\n",
        "**POC Requirements**\n",
        "\n",
        "1. Reading Frames from a video - **Using OpenCV and Decord**\n",
        "2. Finding all Faces in the Frame - **Using Fave_Recognition**\n",
        "3. Clustering the faces into groups by using the distance between facial embeddings - **Feature Extraction using Deep Metric learning with the help of Face_Recogintion built using state of the art face recognition built with  deep learing**\n",
        "4.The next step is **Face Recognition** is to compute the vectors for each image using the same network we used above and then compare the embeddings with the rest of the embeddings we have **(Not part of POC but can be achive using same Face_Recognition)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtAV9sKaeLad"
      },
      "source": [
        "**Step1 : Reading Frames From a video (Upload a video to content folder)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbI3OHzBeV6G"
      },
      "source": [
        "pip install decord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS7EQDldp6-v"
      },
      "source": [
        "import cv2  # still used to save images out\n",
        "import os\n",
        "import numpy as np\n",
        "from decord import VideoReader\n",
        "from decord import cpu, gpu\n",
        "\n",
        "\n",
        "def get_frames(video_path, frames_dir, overwrite=False, start=-1, end=-1, every=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video using decord's VideoReader\n",
        "    :param video_path: path of the video\n",
        "    :param frames_dir: the directory to save the frames\n",
        "    :param overwrite: to overwrite frames that already exist?\n",
        "    :param start: start frame\n",
        "    :param end: end frame\n",
        "    :param every: frame spacing\n",
        "    :return: count of images saved\n",
        "    \"\"\"\n",
        "\n",
        "    video_path = os.path.normpath(video_path)  # make the paths OS (Windows) compatible\n",
        "    frames_dir = os.path.normpath(frames_dir)  # make the paths OS (Windows) compatible\n",
        "\n",
        "    video_dir, video_filename = os.path.split(video_path)  # get the video path and filename from the path\n",
        "\n",
        "    assert os.path.exists(video_path)  # assert the video file exists\n",
        "\n",
        "    # load the VideoReader\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))  # can set to cpu or gpu .. ctx=gpu(0)\n",
        "                     \n",
        "    if start < 0:  # if start isn't specified lets assume 0\n",
        "        start = 0\n",
        "    if end < 0:  # if end isn't specified assume the end of the video\n",
        "        end = len(vr)\n",
        "\n",
        "    frames_list = list(range(start, end, every))\n",
        "    saved_count = 0\n",
        "\n",
        "    if every > 25 and len(frames_list) < 1000:  # this is faster for every > 25 frames and can fit in memory\n",
        "        frames = vr.get_batch(frames_list).asnumpy()\n",
        "        print(\"frames\")\n",
        "\n",
        "        for index, frame in zip(frames_list, frames):  # lets loop through the frames until the end\n",
        "            save_path = os.path.join(frames_dir, video_filename, \"{:010d}.jpg\".format(index))  # create the save path\n",
        "            if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
        "                cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # save the extracted image\n",
        "                saved_count += 1  # increment our counter by one\n",
        "\n",
        "    else:  # this is faster for every <25 and consumes small memory\n",
        "        for index in range(start, end):  # lets loop through the frames until the end\n",
        "            frame = vr[index]  # read an image from the capture\n",
        "            \n",
        "            if index % every == 0:  # if this is a frame we want to write out based on the 'every' argument\n",
        "                save_path = os.path.join(frames_dir, video_filename, \"{:010d}.jpg\".format(index))  # create the save path\n",
        "                if not os.path.exists(save_path) or overwrite:  # if it doesn't exist or we want to overwrite anyways\n",
        "                    cv2.imwrite(save_path, cv2.cvtColor(frame.asnumpy(), cv2.COLOR_RGB2BGR))  # save the extracted image\n",
        "                    saved_count += 1  # increment our counter by one\n",
        "\n",
        "    return saved_count  # and return the count of the images we saved\n",
        "\n",
        "\n",
        "def frames_from_video(video_path, frames_dir, overwrite=False, every=1):\n",
        "    \"\"\"\n",
        "    Extracts the frames from a video\n",
        "    :param video_path: path to the video\n",
        "    :param frames_dir: directory to save the frames\n",
        "    :param overwrite: overwrite frames if they exist?\n",
        "    :param every: extract every this many frames\n",
        "    :return: path to the directory where the frames were saved, or None if fails\n",
        "    \"\"\"\n",
        "\n",
        "    video_path = os.path.normpath(video_path)  # make the paths OS (Windows) compatible\n",
        "    frames_dir = os.path.normpath(frames_dir)  # make the paths OS (Windows) compatible\n",
        "\n",
        "    video_dir, video_filename = os.path.split(video_path)  # get the video path and filename from the path\n",
        "\n",
        "    # make directory to save frames, its a sub dir in the frames_dir with the video name\n",
        "    os.makedirs(os.path.join(frames_dir, video_filename), exist_ok=True)\n",
        "    \n",
        "    print(\"Extracting frames from {}\".format(video_filename))\n",
        "    \n",
        "    get_frames(video_path, frames_dir, every=every)  # let's now extract the frames\n",
        "\n",
        "    return os.path.join(frames_dir, video_filename)  # when done return the directory containing the frames\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # test it\n",
        "    frames_from_video(video_path='sample.mp4', frames_dir='test_frames', overwrite=False, every=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDcd2eu0e7o6"
      },
      "source": [
        "**Nowadays computers will in general have various CPU cores that can get things done in parallel. So we can extend out upon the above code to add parallel processing over all CPU cores.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9YMN22pjCJs"
      },
      "source": [
        "**Step 2 : Face Detection and Feature Extraction using Deep metric learning (Vectors or facial embeddings)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EGy0a21nXe6"
      },
      "source": [
        "**Step 3 : Change Runtime to GPU and create Images folder and upload any of the images that we got form previous step (we will automate this in actual implementation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uko0j6tyf8Az"
      },
      "source": [
        "pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqrjsu8aSkdr"
      },
      "source": [
        "from imutils import paths\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "import cv2\n",
        "import os\n",
        " \n",
        "#get paths of each file in folder named Images\n",
        "#Images here contains my data(folders of various persons)\n",
        "imagePaths = list(paths.list_images('Images'))\n",
        "knownEncodings = []\n",
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "    # load the input image and convert it from BGR (OpenCV ordering)\n",
        "    # to dlib ordering (RGB)\n",
        "    image = cv2.imread(imagePath)\n",
        "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    #Use Face_recognition to locate faces\n",
        "    boxes = face_recognition.face_locations(rgb,model='hog')\n",
        "    print(boxes)\n",
        "    top, right, bottom, left = boxes[0]\n",
        "    face_image = image[top:bottom, left:right]\n",
        "    plt.imshow(face_image)\n",
        "    # compute the facial embedding for the face\n",
        "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
        "    # loop over the encodings\n",
        "    for encoding in encodings:\n",
        "        knownEncodings.append(encoding)\n",
        "#save emcodings along with their names in dictionary data\n",
        "data = {\"encodings\": knownEncodings}\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn3LPdTqjXSC"
      },
      "source": [
        "**Step 3: Face Recognition by comparing the vectors or facial embeddings (Not part of POC)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLCE5j2IjlxF"
      },
      "source": [
        "**We can use Google Vertex AI to save , deploy and to inference. i played with Vertex AI for some time and planned to deploy there but a billing account should be linked with the project so couldnt able to do that for POC**"
      ]
    }
  ]
}